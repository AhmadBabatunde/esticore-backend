"""
Agent tools for floor plan processing and annotation
"""
import os
import json
import uuid
import tempfile
import tiktoken
from typing import List, Dict, Tuple
from PIL import Image, ImageDraw, ImageFont
from pdf2image import convert_from_path
from pypdf import PdfReader, PdfWriter
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
from inference_sdk import InferenceHTTPClient
from langchain_tavily import TavilySearch
from datetime import datetime

from modules.config.settings import settings
from modules.pdf_processing.service import pdf_processor

# Initialize Roboflow client
CLIENT = InferenceHTTPClient(
    api_url=settings.ROBOFLOW_API_URL,
    api_key=settings.ROBOFLOW_API_KEY
)

# Initialize tokenizer for chunking
# Use model name from settings so tokenizer model can be configured via OPENAI_MODEL
tokenizer = tiktoken.encoding_for_model(settings.OPENAI_MODEL)

def estimate_tokens(text: str) -> int:
    """Estimate the number of tokens in a text string."""
    return len(tokenizer.encode(text))

def chunk_context_for_processing(context: str, question: str, max_chunk_tokens: int = 4000) -> List[Dict[str, str]]:
    """Split large context into manageable chunks for processing."""
    # Reserve tokens for question, prompt template, and response
    system_overhead = estimate_tokens(f"""Based on the following context from the document, answer the user's question.
    
    Context:
    
    User question: {question}
    
    Provide a helpful and accurate answer:""")
    
    available_tokens = max_chunk_tokens - system_overhead - 500  # 500 tokens buffer for response
    
    if estimate_tokens(context) <= available_tokens:
        return [{"chunk": context, "chunk_id": 1, "total_chunks": 1}]
    
    # Split context into smaller pieces
    lines = context.split('\n\n')
    chunks = []
    current_chunk = ""
    chunk_id = 1
    
    for line in lines:
        test_chunk = current_chunk + "\n\n" + line if current_chunk else line
        
        if estimate_tokens(test_chunk) <= available_tokens:
            current_chunk = test_chunk
        else:
            if current_chunk:
                chunks.append({
                    "chunk": current_chunk,
                    "chunk_id": chunk_id,
                    "total_chunks": 0  # Will be updated later
                })
                chunk_id += 1
                current_chunk = line
            else:
                # Single line is too long, need to split it further
                words = line.split(' ')
                temp_chunk = ""
                for word in words:
                    test_word_chunk = temp_chunk + " " + word if temp_chunk else word
                    if estimate_tokens(test_word_chunk) <= available_tokens:
                        temp_chunk = test_word_chunk
                    else:
                        if temp_chunk:
                            chunks.append({
                                "chunk": temp_chunk,
                                "chunk_id": chunk_id,
                                "total_chunks": 0
                            })
                            chunk_id += 1
                            temp_chunk = word
                        else:
                            # Single word is too long, truncate it
                            temp_chunk = word[:available_tokens//2]
                            chunks.append({
                                "chunk": temp_chunk,
                                "chunk_id": chunk_id,
                                "total_chunks": 0
                            })
                            chunk_id += 1
                            temp_chunk = ""
                
                if temp_chunk:
                    current_chunk = temp_chunk
    
    if current_chunk:
        chunks.append({
            "chunk": current_chunk,
            "chunk_id": chunk_id,
            "total_chunks": 0
        })
    
    # Update total_chunks count
    total_chunks = len(chunks)
    for chunk in chunks:
        chunk["total_chunks"] = total_chunks
    
    return chunks

def combine_chunk_responses(responses: List[str], question: str) -> str:
    """Combine responses from multiple chunks into a coherent answer."""
    if len(responses) == 1:
        return responses[0]
    
    # Combine all responses
    combined_text = "\n\n".join([f"Section {i+1}: {resp}" for i, resp in enumerate(responses)])
    
    # Use LLM to synthesize the combined responses
    llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)
    synthesis_prompt = f"""I have gathered information from multiple sections of a document to answer this question: {question}

Combined information from all sections:
{combined_text}

Please provide a comprehensive, coherent answer that synthesizes the information from all sections. Remove any redundancy and organize the information logically:"""
    
    try:
        synthesis_response = llm.invoke(synthesis_prompt)
        return synthesis_response.content
    except Exception as e:
        print(f"DEBUG: Error in synthesis, returning combined text: {e}")
        return f"Based on the document analysis:\n\n" + "\n\n".join(responses)

def get_internet_search_results(query: str, max_results: int = 3) -> str:
    """Get internet search results for supplementary information."""
    try:
        # Initialize Tavily search tool with configuration from settings
        tavily_search = TavilySearch(
            max_results=max_results,
            topic="general",
            include_answer=True,
            include_raw_content=False,
            include_images=False
        )
        
        # Execute the search
        result = tavily_search.invoke({"query": query})
        
        # Format the results for inclusion in answers
        search_results = result.get("results", [])
        if not search_results:
            return ""
        
        formatted_results = "Additional context from current information:\n"
        for i, item in enumerate(search_results[:max_results], 1):
            title = item.get("title", "No title")
            content = item.get("content", "No content")
            url = item.get("url", "")
            
            formatted_results += f"\n{i}. {title}\n"
            formatted_results += f"   {content[:300]}..." if len(content) > 300 else f"   {content}"
            if url:
                formatted_results += f"\n   Source: {url}"
            formatted_results += "\n"
        
        return formatted_results
        
    except Exception as e:
        print(f"DEBUG: Internet search failed: {e}")
        return ""

def create_comprehensive_answer(doc_content: str, web_content: str, question: str, citations: List = None) -> str:
    """Create a comprehensive answer combining document content and web information."""
    try:
        # Prepare the content for the LLM
        combined_context = ""
        
        if doc_content:
            combined_context += f"DOCUMENT INFORMATION:\n{doc_content}\n\n"
        
        if web_content:
            combined_context += f"CURRENT INFORMATION:\n{web_content}\n\n"
        
        # If no content from either source, provide a helpful response
        if not combined_context.strip():
            return f"I understand you're asking about: {question}. Let me provide what I can tell you about this topic based on general knowledge and analysis capabilities. I can analyze both textual content and visual elements (layouts, diagrams, spatial relationships) when available. However, I recommend consulting current resources, expert documentation, and specialized sources for the most accurate and up-to-date information."
        
        # Create citation references if available
        citation_text = ""
        if citations:
            citation_text = "\n\nDocument citations:\n"
            for citation in citations[:5]:
                citation_text += f"[{citation['id']}] Page {citation['page']}\n"
        
        # Use LLM to create comprehensive answer
        llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)
        comprehensive_prompt = f"""Based on the following information sources, provide a comprehensive and detailed answer to the user's question. Synthesize information from both the document content (including any visual analysis) and current web sources to give the most complete response possible.

QUESTION: {question}

AVAILABLE INFORMATION:
{combined_context}

Please provide a thorough, well-organized answer that:
1. Directly addresses the question
2. Combines relevant information from all sources (document text, visual analysis, and web content)
3. Provides specific details and examples where available
4. Offers practical insights and recommendations
5. Integrates visual information (layouts, designs, spatial relationships) when relevant
6. Maintains accuracy while being comprehensive

If some aspects of the question cannot be fully answered from the available sources, acknowledge this but still provide all relevant information that is available.{citation_text}"""
        
        response = llm.invoke(comprehensive_prompt)
        return response.content
        
    except Exception as e:
        print(f"DEBUG: Error creating comprehensive answer: {e}")
        # Fallback to combining the content directly
        fallback_answer = f"Based on the available information regarding '{question}':\n\n"
        if doc_content:
            fallback_answer += f"From the document: {doc_content}\n\n"
        if web_content:
            fallback_answer += f"Current information: {web_content}\n\n"
        return fallback_answer or f"I understand you're asking about {question}. This appears to be an important topic that would benefit from consulting current expert sources and documentation."

def should_use_internet_search(question: str) -> bool:
    """Determine if a question requires internet search based on keywords and context."""
    question_lower = question.lower()
    
    # Greetings and simple interactions - no search needed
    greeting_patterns = ['hello', 'hi', 'hey', 'good morning', 'good afternoon', 'good evening', 'how are you', 'thanks', 'thank you']
    if any(greeting in question_lower for greeting in greeting_patterns):
        return False
    
    # Keywords that indicate need for current/recent information
    current_info_keywords = [
        'current', 'recent', 'latest', 'new', 'updated', 'today', 'now', 'this year', 
        'market trends', 'news', 'regulations', 'standards', 'prices', 'cost', 
        'what is happening', 'what happened', 'recent developments', 'updates'
    ]
    
    # Keywords that indicate document-based questions
    document_keywords = [
        'page', 'document', 'pdf', 'floor plan', 'drawing', 'diagram', 'layout',
        'what does this show', 'what is on', 'describe', 'analyze', 'explain this'
    ]
    
    # If question contains document-specific keywords, don't search internet
    if any(keyword in question_lower for keyword in document_keywords):
        return False
    
    # If question explicitly asks for current information, search internet
    if any(keyword in question_lower for keyword in current_info_keywords):
        return True
    
    # Default: don't search internet unless explicitly needed
    return False

def process_question_with_hybrid_search(doc_id: str, question: str, include_suggestions: bool = False) -> Dict:
    """Process question using both document RAG and internet search for comprehensive answers with concurrent processing."""
    doc_content = ""
    web_content = ""
    citations = []
    most_referenced_page = None
    suggestions = []
    
    import concurrent.futures
    import threading
    
    # Results containers for concurrent operations
    doc_result = {"content": "", "citations": [], "most_referenced_page": None}
    web_result = {"content": ""}
    
    # Check if internet search is needed
    needs_internet_search = should_use_internet_search(question)
    
    def fetch_document_content():
        """Fetch document content in a separate thread with image analysis support."""
        try:
            print(f"DEBUG: Retrieving document content for: {question}")
            docs = None
            if getattr(pdf_processor, 'use_database_storage', False):
                docs = pdf_processor.query_document_vectors(doc_id, question, k=8)
                # docs is a list of dicts with 'page' and 'text'
            else:
                vs = pdf_processor.load_vectorstore(doc_id)
                docs = vs.similarity_search(question, k=8)
                # docs is a list of objects with .metadata and .page_content

            if docs:
                # Create citations from docs
                doc_citations = []
                for i, doc in enumerate(docs):
                    if isinstance(doc, dict):
                        doc_citations.append({
                            "id": i + 1,
                            "page": doc.get("page", 1),
                            "text": doc.get("text", "")[:500] if doc.get("text") else "",
                            "relevance_score": 0.8,
                            "doc_id": doc_id
                        })
                    else:
                        doc_citations.append({
                            "id": i + 1,
                            "page": doc.metadata.get("page", 1),
                            "text": doc.page_content[:500] if doc.page_content else "",
                            "relevance_score": 0.8,
                            "doc_id": doc_id
                        })
                # Find most referenced page
                page_counts = {}
                for citation in doc_citations:
                    page = citation["page"]
                    page_counts[page] = page_counts.get(page, 0) + 1
                doc_most_referenced = None
                if page_counts:
                    doc_most_referenced = max(page_counts.items(), key=lambda x: x[1])[0]
                
                # Format document content
                context = "\n\n".join([f"Page {d.metadata.get('page', 'N/A')}: {d.page_content}" for d in docs])
                
                # Check if visual analysis is needed for pages with minimal text
                visual_analysis_needed = any(keyword in question.lower() for keyword in [
                    'layout', 'arrangement', 'position', 'where', 'located', 'diagram', 'drawing', 
                    'plan', 'design', 'visual', 'look', 'appearance', 'orientation', 'spatial', 
                    'show', 'see', 'view', 'display', 'illustrate', 'color', 'shape', 'size'
                ])
                
                multimodal_analysis = ""
                if visual_analysis_needed:
                    print(f"DEBUG: Visual question detected, checking pages for image analysis")
                    # Get unique pages from docs
                    relevant_pages = list(set(d.metadata.get('page', 1) for d in docs[:4]))  # Limit to 4 pages for speed
                    
                    try:
                        doc_info = pdf_processor.get_document_info(doc_id)
                        reader = PdfReader(doc_info["pdf_path"])
                        
                        # Check which pages need visual analysis (minimal text)
                        pages_needing_visual = []
                        for page_num in relevant_pages[:2]:  # Limit to 2 pages for speed
                            try:
                                if page_num <= len(reader.pages):
                                    page = reader.pages[page_num - 1]
                                    raw_text = page.extract_text()
                                    if not raw_text or len(raw_text.strip()) < 50 or '[No text extracted:' in raw_text:
                                        pages_needing_visual.append(page_num)
                            except Exception as e:
                                print(f"DEBUG: Error checking page {page_num} for visual analysis: {e}")
                        
                        # Perform visual analysis on pages that need it (max 1 for speed)
                        if pages_needing_visual:
                            print(f"DEBUG: Performing visual analysis on {len(pages_needing_visual[:1])} pages")
                            for page_num in pages_needing_visual[:1]:
                                try:
                                    analysis = analyze_pdf_page_multimodal(doc_id, page_num)
                                    multimodal_analysis += f"\n\nVisual analysis of page {page_num}:\n{analysis}"
                                except Exception as e:
                                    print(f"DEBUG: Error in visual analysis for page {page_num}: {e}")
                                    continue
                    except Exception as e:
                        print(f"DEBUG: Error in visual analysis setup: {e}")
                
                # Combine text and visual content
                enhanced_context = context
                if multimodal_analysis:
                    enhanced_context += f"\n\nAdditional visual insights:{multimodal_analysis}"
                
                # Handle chunking if necessary (smaller chunks for speed)
                chunks = chunk_context_for_processing(enhanced_context, question, max_chunk_tokens=3000)
                
                if len(chunks) == 1:
                    doc_result["content"] = chunks[0]['chunk']
                else:
                    # Process multiple chunks quickly
                    chunk_responses = []
                    llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)
                    
                    for chunk_info in chunks[:3]:  # Limit to 3 chunks for speed
                        prompt = f"""Extract key information from this document section for: {question}

Section:
{chunk_info['chunk']}

Provide only relevant information (max 2 sentences). If no relevant info, respond "No relevant information.":"""
                        
                        try:
                            chunk_response = llm.invoke(prompt)
                            if "No relevant information" not in chunk_response.content:
                                chunk_responses.append(chunk_response.content)
                        except Exception as e:
                            print(f"DEBUG: Error processing document chunk: {e}")
                            continue
                    
                    if chunk_responses:
                        doc_result["content"] = "\n\n".join(chunk_responses)
                
                doc_result["citations"] = doc_citations
                doc_result["most_referenced_page"] = doc_most_referenced
                
        except Exception as e:
            print(f"DEBUG: Error retrieving document content: {e}")
    
    def fetch_web_content():
        """Fetch web content in a separate thread - only if needed."""
        try:
            if needs_internet_search:
                print(f"DEBUG: Searching internet for: {question}")
                web_result["content"] = get_internet_search_results(question, max_results=2)  # Reduced for speed
            else:
                print(f"DEBUG: Skipping internet search for: {question}")
        except Exception as e:
            print(f"DEBUG: Error fetching web content: {e}")
    
    try:
        # Execute operations - concurrent only if internet search is needed
        if needs_internet_search:
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                # Submit both tasks
                doc_future = executor.submit(fetch_document_content)
                web_future = executor.submit(fetch_web_content)
                
                # Wait for both to complete with timeout for speed
                try:
                    concurrent.futures.wait([doc_future, web_future], timeout=10.0)  # 10 second timeout
                except concurrent.futures.TimeoutError:
                    print("DEBUG: Timeout in concurrent processing, proceeding with available results")
        else:
            # Only fetch document content
            fetch_document_content()
        
        # Extract results
        doc_content = doc_result["content"]
        web_content = web_result["content"]
        citations = doc_result["citations"]
        most_referenced_page = doc_result["most_referenced_page"]
        
        # Generate suggestions quickly if requested
        if include_suggestions and citations:
            relevant_pages = list(set(c["page"] for c in citations[:3]))  # Reduced for speed
            for i, page_num in enumerate(relevant_pages[:2]):  # Max 2 suggestions for speed
                suggestions.append({
                    "title": f"Page {page_num} Details",
                    "page": page_num,
                    "description": f"Additional information on page {page_num}."
                })
        
        # Create comprehensive answer with timeout protection
        try:
            comprehensive_answer = create_comprehensive_answer(doc_content, web_content, question, citations)
        except Exception as e:
            print(f"DEBUG: Error in comprehensive answer generation: {e}")
            # Quick fallback
            comprehensive_answer = f"Based on available information regarding '{question}': "
            if doc_content:
                comprehensive_answer += f"From the document: {doc_content[:500]}... "
            if web_content:
                comprehensive_answer += f"Current information: {web_content[:500]}..."
            if not doc_content and not web_content:
                comprehensive_answer += "This topic requires further research from current expert sources and documentation."
        
        return {
            "answer": comprehensive_answer,
            "suggestions": suggestions,
            "citations": citations,
            "most_referenced_page": most_referenced_page,
            "has_document_content": bool(doc_content),
            "has_web_content": bool(web_content)
        }
        
    except Exception as e:
        print(f"DEBUG: Error in hybrid search processing: {e}")
        # Ultra-fast fallback response
        fallback_answer = f"I understand you're asking about: {question}. This is an important topic. Based on general knowledge and best practices, I can provide that this involves multiple considerations including technical specifications, practical factors, regulatory requirements, and current standards. For the most comprehensive and accurate information, I recommend consulting current expert sources, documentation, and specialized resources relevant to your specific context."
        
        # Try to get basic citations even in fallback
        fallback_citations = []
        fallback_most_referenced = None
        try:
            # Attempt basic document search for citations
            if getattr(pdf_processor, 'use_database_storage', False):
                docs = pdf_processor.query_document_vectors(doc_id, question, k=3)
                for i, doc in enumerate(docs[:3]):
                    fallback_citations.append({
                        "id": i + 1,
                        "page": doc.get("page", 1),
                        "text": doc.get("text", "")[:200] + "...",
                        "relevance_score": 0.5,
                        "doc_id": doc_id
                    })
                if fallback_citations:
                    fallback_most_referenced = fallback_citations[0]["page"]
            else:
                vs = pdf_processor.load_vectorstore(doc_id)
                docs = vs.similarity_search(question, k=3)
                for i, doc in enumerate(docs[:3]):
                    fallback_citations.append({
                        "id": i + 1,
                        "page": doc.metadata.get("page", 1),
                        "text": doc.page_content[:200] + "...",
                        "relevance_score": 0.5,
                        "doc_id": doc_id
                    })
                if fallback_citations:
                    fallback_most_referenced = fallback_citations[0]["page"]
        except Exception as e:
            print(f"DEBUG: Could not get fallback citations: {e}")
        
        return {
            "answer": fallback_answer,
            "suggestions": [],
            "citations": fallback_citations,
            "most_referenced_page": fallback_most_referenced,
            "has_document_content": bool(fallback_citations),
            "has_web_content": False
        }

@tool
def load_pdf_for_floorplan(pdf_path: str) -> str:
    """Load and validate a PDF file for floor plan processing."""
    try:
        if not os.path.exists(pdf_path):
            return f"Error: PDF file not found at '{pdf_path}'."

        # Try to convert first page to verify PDF is readable
        images = convert_from_path(pdf_path, dpi=150, first_page=1, last_page=1)
        if not images:
            return f"Error: Could not read PDF pages from '{pdf_path}'."

        return f"PDF '{pdf_path}' loaded successfully and is ready for floor plan processing."
    except Exception as e:
        return f"Error loading PDF: {str(e)}"

@tool
def convert_pdf_page_to_image(pdf_path: str, page: int = 1, dpi: int = 300) -> str:
    """Convert a specific page of a PDF to a temporary image file for processing."""
    try:
        if not os.path.exists(pdf_path):
            return f"Error: PDF file not found at '{pdf_path}'."

        print(f"DEBUG: Converting PDF page {page} to image with DPI {dpi}")
        images = convert_from_path(pdf_path, dpi=dpi, first_page=page, last_page=page)

        if not images:
            return f"Error: Page {page} not found in PDF."

        temp_image_path = f"temp_floor_plan_page_{page}.png"
        image = images[0]
        image.save(temp_image_path, "PNG")

        width, height = image.size

        print(f"DEBUG: Temporary image saved to {temp_image_path}")
        return json.dumps({
            "success": True,
            "image_path": temp_image_path,
            "page": page,
            "width": width,
            "height": height,
            "dpi": dpi
        })
    except Exception as e:
        return json.dumps({"success": False, "error": f"Error converting PDF to image: {str(e)}"})

@tool
def detect_floor_plan_objects(image_path: str = "temp_floor_plan.png", conf_threshold: float = 0.38) -> str:
    """Detect all objects in the floor plan image using a Roboflow model and return a JSON list of objects."""
    try:
        if not os.path.exists(image_path):
            return f"Error: Image file not found at '{image_path}'."

        print(f"DEBUG: Running Roboflow inference on {image_path}")

        # Run inference
        result = CLIENT.infer(image_path, model_id=settings.ROBOFLOW_MODEL_ID)

        detected_objects = []
        for pred in result.get("predictions", []):
            # Convert from (x_center, y_center, width, height) to (x1, y1, x2, y2)
            x_center, y_center = pred["x"], pred["y"]
            w, h = pred["width"], pred["height"]
            x1, y1 = int(x_center - w / 2), int(y_center - h / 2)
            x2, y2 = int(x_center + w / 2), int(y_center + h / 2)

            obj = {
                "bbox": [x1, y1, x2, y2],
                "class_name": pred["class"],
                "confidence": round(pred["confidence"], 2),
                "class_id": pred["class_id"],
            }
            detected_objects.append(obj)

        print(f"DEBUG: Detected {len(detected_objects)} objects")
        return json.dumps(detected_objects, indent=2)
    except Exception as e:
        return f"Error during detection: {str(e)}"

@tool
def generate_frontend_annotations(
    objects_json: str, 
    page_number: int, 
    annotation_type: str, 
    image_path: str,
    dpi: int,
    filter_condition: str = ""
) -> str:
    """
    Generates a JSON object of annotations for detected objects, designed to be rendered on the frontend with a library like pdf.js.

    Args:
        objects_json: JSON string of detected objects from detect_floor_plan_objects.
        page_number: The page number where the annotations should be applied.
        annotation_type: The type of annotation to generate. Supported types: 'highlight', 'rectangle', 'circle', 'count', 'arrow'.
        image_path: The path to the image that was used for detection.
        dpi: The DPI of the image that was used for detection.
        filter_condition: A string to filter which objects to annotate (e.g., 'door', 'window'). Annotates all objects if empty.
    
    Returns:
        A JSON string containing the list of annotation objects and the list of all detected objects.
    """
    try:
        all_objects = json.loads(objects_json)
        if not isinstance(all_objects, list):
            return json.dumps({"error": "Objects data must be a list of detected objects."})

        # Get image dimensions for coordinate space metadata
        with Image.open(image_path) as img:
            width, height = img.size

        # Filter objects
        if filter_condition:
            objects_to_annotate = [
                obj for obj in all_objects
                if filter_condition.lower() in obj.get('class_name', '').lower()
            ]
            if not objects_to_annotate:
                available = sorted(list(set(obj.get('class_name', 'N/A') for obj in all_objects)))
                return json.dumps({
                    "error": f"No objects found matching filter '{filter_condition}'.",
                    "available_classes": available
                })
        else:
            objects_to_annotate = all_objects

        # Map simple types to pdf.js tool IDs
        tool_id_map = {
            'highlight': 'markup.highlight',
            'rectangle': 'shape.rectangle',
            'circle': 'shape.ellipse', # pdf.js uses ellipse for circles
            'count': 'text.free', # Use free text to display a number
            'arrow': 'shape.arrow'
        }
        
        # Default properties
        color_map = {
            'highlight': '#FFFF00', # Yellow
            'rectangle': '#FF0000', # Red
            'circle': '#0000FF',    # Blue
            'count': '#00FF00',     # Green
            'arrow': '#FF00FF'      # Magenta
        }

        tool_id = tool_id_map.get(annotation_type.lower())
        if not tool_id:
            return json.dumps({"error": f"Unsupported annotation type: '{annotation_type}'. Supported types are: {list(tool_id_map.keys())}"})

        annotations = []
        for i, obj in enumerate(objects_to_annotate, 1):
            bbox = obj['bbox']
            x1, y1, x2, y2 = bbox
            obj_width = x2 - x1
            obj_height = y2 - y1
            
            annotation = {
                "id": str(uuid.uuid4()),
                "tool": tool_id,
                "x": float(x1),
                "y": float(y1),
                "width": float(obj_width),
                "height": float(obj_height),
                "color": color_map.get(annotation_type.lower(), "#000000"),
                "lineWidth": 2.0,
                "timestamp": int(datetime.now().timestamp()),
                "page": page_number,
                "text": f"{obj['class_name']} ({obj['confidence']:.2f})"
            }

            if annotation_type.lower() == 'circle':
                # For pdf.js ellipse, x/y is top-left, width/height defines the bounding box
                pass

            if annotation_type.lower() == 'count':
                annotation['text'] = str(i)
                # make the count "box" smaller and centered
                annotation['width'] = 20.0
                annotation['height'] = 20.0
                annotation['x'] = float(x1 + (obj_width / 2) - 10)
                annotation['y'] = float(y1 + (obj_height / 2) - 10)
            
            if annotation_type.lower() == 'arrow':
                center_x = x1 + obj_width / 2
                center_y = y1 + obj_height / 2
                start_x = x1 - 30 if x1 > 30 else x1 + obj_width + 30
                start_y = y1 - 30 if y1 > 30 else y1 + obj_height + 30
                annotation['points'] = [float(start_x), float(start_y), float(center_x), float(center_y)]
                del annotation['x'], annotation['y'], annotation['width'], annotation['height']

            annotations.append(annotation)

        response_data = {
            "annotations": annotations,
            "detected_objects": all_objects,
            "message": f"Generated {len(annotations)} annotations of type '{annotation_type}' successfully.",
            "coordinate_space": {
                "system": "pixel",
                "origin": "top-left",
                "reference": "rasterized_page",
                "width": width,
                "height": height,
                "dpi": dpi,
                "rotation": 0
            }
        }
        
        return json.dumps(response_data, indent=2)
    except json.JSONDecodeError:
        return json.dumps({"error": "Invalid JSON format for detected objects."})
    except Exception as e:
        return json.dumps({"error": f"Error generating annotations: {str(e)}"})
    finally:
        # Clean up the temporary image file
        if os.path.exists(image_path):
            try:
                os.remove(image_path)
                print(f"DEBUG: Cleaned up temporary image file: {image_path}")
            except Exception as e:
                print(f"DEBUG: Error cleaning up temporary image file {image_path}: {e}")

@tool
def verify_detections(image_path: str, objects_json: str, requested_object: str) -> str:
    """Verify if the detected objects list contains the requested object type."""
    try:
        if not os.path.exists(image_path):
            return "Error: Image file not found."

        if not objects_json:
            return "Error: No objects data provided to verify."

        try:
            detected_objects = json.loads(objects_json)
            if not isinstance(detected_objects, list):
                return "Error: Objects data must be a list of detected objects."
        except json.JSONDecodeError:
            return "Error: Invalid JSON format for objects data."

        class_counts = {}
        for obj in detected_objects:
            cls = obj.get('class_name', 'unknown').lower()
            class_counts[cls] = class_counts.get(cls, 0) + 1

        response = {
            "requested_object_found": False,
            "class_counts": class_counts,
            "message": "",
            "suggested_filter_condition": ""
        }

        requested_object_lower = requested_object.lower()
        found_classes = []
        
        # Check for exact matches and partial matches
        for cls in class_counts.keys():
            if requested_object_lower == cls:
                # Exact match
                found_classes.append(cls)
                break
            elif requested_object_lower in cls or cls in requested_object_lower:
                # Partial match
                found_classes.append(cls)
        
        if found_classes:
            response['requested_object_found'] = True
            matched_class = found_classes[0]
            count = class_counts[matched_class]
            response['message'] = f"Verification successful: Found {count} objects matching '{requested_object}' (class: '{matched_class}')."
            response['suggested_filter_condition'] = matched_class
        else:
            # No matches found
            response['message'] = f"Verification failed: No objects matching '{requested_object}' were detected. Available classes: {sorted(list(class_counts.keys()))}"
            # Try to suggest the most similar class name
            if class_counts:
                # Find the most similar class name
                import difflib
                closest_matches = difflib.get_close_matches(requested_object_lower, class_counts.keys(), n=3, cutoff=0.3)
                if closest_matches:
                    response['message'] += f"\n\nDid you mean one of these? {closest_matches}"

        return json.dumps(response, indent=2)

    except Exception as e:
        return f"Error verifying detections: {str(e)}"

@tool
def answer_question_using_rag(doc_id: str, question: str) -> str:
    """Answer questions using document content only - simple and fast."""
    try:
        print(f"DEBUG: Processing question with document-only approach: {question}")
        docs = None
        if getattr(pdf_processor, 'use_database_storage', False):
            docs = pdf_processor.query_document_vectors(doc_id, question, k=4)
            # docs is a list of dicts with 'page' and 'text'
            context = "\n\n".join([f"Page {d.get('page', 'N/A')}: {d.get('text', '')}" for d in docs[:3]])
        else:
            vs = pdf_processor.load_vectorstore(doc_id)
            docs = vs.similarity_search(question, k=4)
            # docs is a list of objects with .metadata and .page_content
            context = "\n\n".join([f"Page {d.metadata.get('page', 'N/A')}: {d.page_content}" for d in docs[:3]])

        if not docs:
            return "I couldn't find any relevant information in the document to answer your question."

        # Use LLM to generate a response based on the context
        llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)
        prompt = f"""Based on the following context from the document, answer the user's question concisely.

Context:
{context}

User question: {question}

Provide a helpful and accurate answer:"""

        rag_response = llm.invoke(prompt)
        return rag_response.content

    except Exception as e:
        print(f"DEBUG: Error in document RAG: {e}")
        return f"I encountered an error while processing your question. Please try rephrasing your question or check if the document is properly loaded."

@tool
def analyze_pdf_page_multimodal(doc_id: str, page_number: int = 1) -> str:
    """Optimized multimodal analysis of a PDF page using both text and visual analysis."""
    try:
        # Get document info to find PDF path
        doc_info = pdf_processor.get_document_info(doc_id)
        
        # Handle different storage types
        pdf_path = None
        if doc_info.get("storage_type") == "database":
            # For database storage, we need to get the PDF content and create a temporary file
            try:
                pdf_content = pdf_processor.get_document_content(doc_id)
                # Create a temporary file for processing
                import tempfile
                with tempfile.NamedTemporaryFile(suffix='.pdf', delete=False) as temp_file:
                    temp_file.write(pdf_content)
                    pdf_path = temp_file.name
            except Exception as e:
                return f"Error retrieving document from database: {str(e)}"
        else:
            # For file storage
            pdf_path = doc_info.get("pdf_path")
        
        if not pdf_path or not os.path.exists(pdf_path):
            return f"Error: PDF file not found for document {doc_id}"
            
        # OPTIMIZATION: Use lower DPI for faster processing (200 instead of 300)
        print(f"DEBUG: Converting page {page_number} to image for multimodal analysis (optimized)")
        images = convert_from_path(pdf_path, dpi=200, first_page=page_number, last_page=page_number)
        
        if not images:
            return f"Error: Page {page_number} not found in PDF."
            
        temp_image_path = f"temp_multimodal_page_{page_number}_{uuid.uuid4().hex[:8]}.png"
        images[0].save(temp_image_path, "PNG")
        print(f"DEBUG: Saved temporary image: {temp_image_path}")
        
        # Extract text from the specified page using pypdf
        reader = PdfReader(pdf_path)
        if page_number > len(reader.pages):
            return f"Error: Page {page_number} does not exist in the document (total pages: {len(reader.pages)})"
            
        # Get the page object
        page = reader.pages[page_number - 1]
        
        # Extract text and check if it's empty or just indicates no text was extracted
        raw_text = page.extract_text()
        if not raw_text or '[No text extracted:' in raw_text:
            page_text = "This page appears to contain primarily visual elements such as diagrams, drawings, or images. No machine-readable text could be extracted from this page." 
        else:
            page_text = raw_text
        
        # Use multimodal LLM to analyze both image and text
        llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)

        # OPTIMIZATION: Shorter, more focused prompt for faster processing
        message_content = [
            {
                "type": "text",
                "text": f"Analyze this document page. Describe the main visual elements, layout, rooms, doors, windows, fixtures, dimensions, labels, and text. Be concise but comprehensive.\n\nExtracted text: {page_text[:500]}\n\nProvide a focused analysis of the key elements and their relationships."
            },
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{encode_image(temp_image_path)}"}
            }
        ]
        
        message = HumanMessage(content=message_content)
        response = llm.invoke([message])
        
        # Clean up temporary image file
        try:
            if os.path.exists(temp_image_path):
                os.remove(temp_image_path)
                print(f"DEBUG: Cleaned up temporary image: {temp_image_path}")
        except Exception as cleanup_error:
            print(f"DEBUG: Could not clean up temporary image {temp_image_path}: {cleanup_error}")
        
        # Clean up temporary PDF file if it was created for database storage
        if doc_info.get("storage_type") == "database" and pdf_path:
            try:
                if os.path.exists(pdf_path):
                    os.remove(pdf_path)
                    print(f"DEBUG: Cleaned up temporary PDF file: {pdf_path}")
            except Exception as pdf_cleanup_error:
                print(f"DEBUG: Error cleaning up temporary PDF file: {pdf_cleanup_error}")
        
        return response.content
        
    except Exception as e:
        return f"Error analyzing PDF page: {str(e)}"

def encode_image(image_path):
    """Encode image to base64 string"""
    import base64
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

@tool
def answer_question_with_suggestions(doc_id: str, question: str) -> str:
    """Answer questions about the document using simple RAG with suggestions - no hybrid approach."""
    try:
        print(f"DEBUG: Processing question with document-only approach: {question}")
        
        # Use simple document search only
        docs = None
        if getattr(pdf_processor, 'use_database_storage', False):
            docs = pdf_processor.query_document_vectors(doc_id, question, k=6)
            context = "\n\n".join([f"Page {d.get('page', 'N/A')}: {d.get('text', '')}" for d in docs[:4]])
        else:
            vs = pdf_processor.load_vectorstore(doc_id)
            docs = vs.similarity_search(question, k=6)
            context = "\n\n".join([f"Page {d.metadata.get('page', 'N/A')}: {d.page_content}" for d in docs[:4]])

        # Generate suggestions and citations
        suggestions = []
        citations = []
        most_referenced_page = None
        if docs:
            for i, d in enumerate(docs[:3]):
                if isinstance(d, dict):
                    page = d.get('page', 1)
                    text = d.get('text', '')
                else:
                    page = getattr(d, 'metadata', {}).get('page', 1) if hasattr(d, 'metadata') else 1
                    text = getattr(d, 'page_content', '')
                
                # Ensure page is a valid number
                if page == 'N/A' or page is None:
                    page = 1
                
                suggestions.append({
                    "title": f"Page {page} Content",
                    "page": page,
                    "description": f"Additional information available on page {page}."
                })
                citations.append({
                    "id": i + 1,
                    "page": page,
                    "text": text[:500] if text else "",
                    "relevance_score": 1.0,
                    "doc_id": doc_id
                })
            most_referenced_page = citations[0]["page"] if citations else None

        # Use LLM to generate a response based on the context
        llm = ChatOpenAI(model=settings.OPENAI_MODEL, temperature=0.0, api_key=settings.OPENAI_API_KEY)
        prompt = f"""Based on the following context from the document, answer the user's question and provide related topic suggestions with page numbers.

Context:
{context}

User question: {question}

Provide a helpful and accurate answer. Include suggestions and cite relevant pages."""
        rag_response = llm.invoke(prompt)

        response_data = {
            "answer": rag_response.content,
            "suggestions": suggestions,
            "citations": citations,
            "most_referenced_page": most_referenced_page,
            "source_info": {
                "has_document_content": bool(docs),
                "has_web_content": False
            }
        }
        return json.dumps(response_data)
        for page_num in relevant_pages[:3]:  # Max 3 suggestions
            suggestions.append({
                "title": f"Page {page_num} Content",
                "page": page_num,
                "description": f"Additional information available on page {page_num}."
            })
        
        response_data = {
            "answer": response_text,
            "suggestions": suggestions,
            "citations": citations,
            "most_referenced_page": most_referenced_page
        }
        
        return json.dumps(response_data)
        
    except Exception as e:
        print(f"DEBUG: Error in document RAG with suggestions: {e}")
        # Try to provide basic citations even in error case
        error_citations = []
        try:
            # Attempt basic document search for citations
            if getattr(pdf_processor, 'use_database_storage', False):
                docs = pdf_processor.query_document_vectors(doc_id, question, k=2)
                for i, doc in enumerate(docs[:2]):
                    error_citations.append({
                        "id": i + 1,
                        "page": doc.get("page", 1),
                        "text": doc.get("text", "")[:200] + "..." if doc.get("text") else "",
                        "relevance_score": 0.5,
                        "doc_id": doc_id
                    })
            else:
                vs = pdf_processor.load_vectorstore(doc_id)
                docs = vs.similarity_search(question, k=2)
                for i, doc in enumerate(docs[:2]):
                    error_citations.append({
                        "id": i + 1,
                        "page": doc.metadata.get("page", 1),
                        "text": doc.page_content[:200] + "..." if doc.page_content else "",
                        "relevance_score": 0.5,
                        "doc_id": doc_id
                    })
        except Exception as citation_error:
            print(f"DEBUG: Could not get error fallback citations: {citation_error}")
        
        # Provide a simple fallback response
        fallback_response = {
            "answer": f"I encountered an error while processing your question about: {question}. Please try rephrasing your question or check if the document is properly loaded.",
            "suggestions": [],
            "citations": error_citations,
            "most_referenced_page": error_citations[0]["page"] if error_citations else None
        }
        return json.dumps(fallback_response)

@tool
def internet_search(query: str) -> str:
    """Search the internet for up-to-date information when needed to answer user queries."""
    try:
        # Initialize Tavily search tool with configuration from settings
        tavily_search = TavilySearch(
            max_results=5,
            topic="general",
            include_answer=True,
            include_raw_content=False,
            include_images=False
        )
        
        # Execute the search
        result = tavily_search.invoke({"query": query})
        
        # Format and return the results
        formatted_result = {
            "query": query,
            "results": result.get("results", []),
            "answer": result.get("answer", "")
        }
        
        return json.dumps(formatted_result)
    except Exception as e:
        return json.dumps({"error": f"Internet search failed: {str(e)}"})

@tool
def measure_objects(image_path: str, objects_json: str, target_object: str, reference_scale: float = None, reference_unit: str = "meters") -> str:
    """
    Measure dimensions of objects in floor plans using pixel-to-real-world conversion.
    
    Args:
        image_path: Path to the floor plan image
        objects_json: JSON string of detected objects from detect_floor_plan_objects
        target_object: The object to measure (e.g., "steel base plate", "door", "window")
        reference_scale: Optional known scale (e.g., if 100 pixels = 1 meter, pass 100)
        reference_unit: Unit of measurement ("meters", "feet", "inches", "millimeters")
    
    Returns:
        JSON string with measurement results
    """
    try:
        import math
        import cv2
        import numpy as np
        
        # Parse detected objects
        detected_objects = json.loads(objects_json)
        if not isinstance(detected_objects, list):
            return json.dumps({"error": "Invalid objects data format"})
        
        # Find target objects
        target_objects = []
        target_lower = target_object.lower()
        
        for obj in detected_objects:
            class_name = obj.get('class_name', '').lower()
            if target_lower in class_name or class_name in target_lower:
                target_objects.append(obj)
        
        if not target_objects:
            available_objects = list(set(obj.get('class_name', 'unknown') for obj in detected_objects))
            return json.dumps({
                "error": f"No '{target_object}' objects found. Available objects: {available_objects}",
                "available_objects": available_objects
            })
        
        # Load image for processing
        image = cv2.imread(image_path)
        if image is None:
            return json.dumps({"error": f"Could not load image from {image_path}"})
        
        height, width = image.shape[:2]
        
        # Calculate scale if not provided
        scale_info = None
        if reference_scale is None:
            scale_info = _auto_detect_scale(image, detected_objects)
        else:
            scale_info = {
                "pixels_per_unit": reference_scale,
                "unit": reference_unit,
                "method": "user_provided",
                "confidence": 1.0
            }
        
        if not scale_info:
            return json.dumps({
                "error": "Could not determine scale automatically. Please provide reference_scale parameter.",
                "suggestion": "Provide a reference scale like: 100 (meaning 100 pixels = 1 meter)"
            })
        
        # Measure each target object
        measurements = []
        
        for i, obj in enumerate(target_objects):
            bbox = obj['bbox']
            x1, y1, x2, y2 = bbox
            
            # Calculate dimensions in pixels
            pixel_width = abs(x2 - x1)
            pixel_height = abs(y2 - y1)
            pixel_area = pixel_width * pixel_height
            diagonal_length = math.sqrt(pixel_width**2 + pixel_height**2)
            
            # Convert to real-world units
            real_width = pixel_width / scale_info["pixels_per_unit"]
            real_height = pixel_height / scale_info["pixels_per_unit"]
            real_area = pixel_area / (scale_info["pixels_per_unit"] ** 2)
            real_diagonal = diagonal_length / scale_info["pixels_per_unit"]
            
            # Apply common object-specific adjustments
            adjusted_measurements = _apply_object_specific_adjustments(
                target_object, real_width, real_height, real_area
            )
            
            measurements.append({
                "object_id": i + 1,
                "class_name": obj.get('class_name', 'unknown'),
                "confidence": obj.get('confidence', 0),
                "pixel_measurements": {
                    "width": round(pixel_width, 2),
                    "height": round(pixel_height, 2),
                    "area": round(pixel_area, 2),
                    "diagonal": round(diagonal_length, 2)
                },
                "real_world_measurements": {
                    "width": round(real_width, 3),
                    "height": round(real_height, 3),
                    "area": round(real_area, 3),
                    "diagonal": round(real_diagonal, 3),
                    "unit": scale_info["unit"]
                },
                "adjusted_measurements": adjusted_measurements,
                "bbox": bbox
            })
        
        # Generate summary
        if measurements:
            widths = [m["real_world_measurements"]["width"] for m in measurements]
            avg_width = sum(widths) / len(widths)
            min_width = min(widths)
            max_width = max(widths)
            
            summary = {
                "total_objects_measured": len(measurements),
                "average_width": round(avg_width, 3),
                "min_width": round(min_width, 3),
                "max_width": round(max_width, 3),
                "measurement_unit": scale_info["unit"]
            }
        else:
            summary = {}
        
        result = {
            "success": True,
            "target_object": target_object,
            "scale_info": scale_info,
            "measurements": measurements,
            "summary": summary,
            "image_dimensions": {"width": width, "height": height}
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": f"Measurement failed: {str(e)}"})

def _auto_detect_scale(image, detected_objects):
    """
    Automatically detect scale by looking for standard-sized objects.
    """
    try:
        import cv2
        import numpy as np
        
        # Common real-world dimensions (in meters)
        STANDARD_DIMENSIONS = {
            'door': 0.9,  # Standard door width
            'window': 1.2,  # Standard window width
            'table': 0.9,   # Standard table width
            'chair': 0.5,   # Standard chair width
            'toilet': 0.4,  # Standard toilet width
        }
        
        # Look for objects with known standard dimensions
        for obj in detected_objects:
            class_name = obj.get('class_name', '').lower()
            
            for standard_obj, standard_width in STANDARD_DIMENSIONS.items():
                if standard_obj in class_name:
                    bbox = obj['bbox']
                    pixel_width = abs(bbox[2] - bbox[0])
                    
                    if pixel_width > 10:  # Ensure reasonable detection
                        pixels_per_meter = pixel_width / standard_width
                        
                        return {
                            "pixels_per_unit": pixels_per_meter,
                            "unit": "meters",
                            "reference_object": class_name,
                            "reference_width_meters": standard_width,
                            "method": f"auto_detected_from_{standard_obj}",
                            "confidence": min(obj.get('confidence', 0.5), 0.8)  # Cap confidence
                        }
        
        # Fallback: Use image dimensions and typical floor plan scales
        height, width = image.shape[:2]
        
        # Assume typical residential floor plan scale
        # For a 10m room in a 1000px image, scale would be 100 px/m
        typical_scale = max(width, height) / 15.0  # Assume 15m for largest dimension
        
        return {
            "pixels_per_unit": typical_scale,
            "unit": "meters",
            "method": "estimated_from_image_size",
            "confidence": 0.3,
            "note": "This is an estimation. For accurate measurements, provide a reference scale."
        }
        
    except Exception as e:
        print(f"DEBUG: Auto-scale detection failed: {e}")
        return None

def _apply_object_specific_adjustments(object_type, width, height, area):
    """
    Apply object-specific measurement adjustments based on typical dimensions.
    """
    object_type_lower = object_type.lower()
    adjustments = {}
    
    # Steel base plate specific adjustments
    if 'steel' in object_type_lower and 'base' in object_type_lower and 'plate' in object_type_lower:
        # Steel base plates are typically square or rectangular with specific thickness
        adjustments.update({
            "likely_thickness_mm": "16-25 mm (typical structural steel)",
            "likely_material": "A36 Steel or equivalent",
            "volume_cm3": round(width * 100 * height * 100 * 0.02, 2),  # Assuming 2cm thickness
            "weight_kg": round(width * height * 0.02 * 7850, 2),  # steel density 7850 kg/m3
            "common_sizes": "150x150mm to 600x600mm for column base plates"
        })
    
    # Door specific adjustments
    elif 'door' in object_type_lower:
        adjustments.update({
            "standard_width_m": 0.9,
            "standard_height_m": 2.1,
            "type": "Interior/Exterior door based on context",
            "swing_clearance": "0.6-0.9m required"
        })
    
    # Window specific adjustments
    elif 'window' in object_type_lower:
        adjustments.update({
            "standard_height_m": 1.2,
            "sill_height_m": "0.9-1.0 typical",
            "type": "Fixed/Casement/Double-hung based on proportions"
        })
    
    # Furniture adjustments
    elif any(furniture in object_type_lower for furniture in ['table', 'desk', 'counter']):
        adjustments.update({
            "standard_height_m": 0.75,
            "typical_depth_m": 0.6 if width > 0.8 else 0.4,
            "usage_notes": "Check for ergonomic clearances"
        })
    
    elif 'chair' in object_type_lower:
        adjustments.update({
            "standard_height_m": 0.45,
            "standard_depth_m": 0.5,
            "clearance_required": "0.6-0.8m behind"
        })
    
    # Add general engineering notes
    adjustments["measurement_notes"] = [
        "Measurements are approximate based on pixel analysis",
        "Verify with actual site measurements for construction",
        "Consider manufacturing tolerances ±2mm",
        "Check local building codes for required dimensions"
    ]
    
    return adjustments

@tool
def calibrate_scale(image_path: str, known_width_pixels: float, known_width_real: float, real_unit: str = "meters") -> str:
    """
    Calibrate the measurement scale using a known dimension.
    
    Args:
        image_path: Path to the floor plan image
        known_width_pixels: Width of a known object in pixels
        known_width_real: Actual width of the object in real units
        real_unit: Unit of measurement ("meters", "feet", "inches", "millimeters")
    
    Returns:
        JSON string with calibration results
    """
    try:
        pixels_per_unit = known_width_pixels / known_width_real
        
        result = {
            "success": True,
            "calibration": {
                "pixels_per_unit": round(pixels_per_unit, 2),
                "unit": real_unit,
                "known_reference": {
                    "pixels": known_width_pixels,
                    "real_units": known_width_real,
                    "unit": real_unit
                }
            },
            "usage_example": f"Use pixels_per_unit={round(pixels_per_unit, 2)} in measure_objects tool",
            "common_conversions": {
                "pixels_per_meter": round(pixels_per_unit, 2) if real_unit == "meters" else "N/A",
                "pixels_per_foot": round(pixels_per_unit / 0.3048, 2) if real_unit == "meters" else "N/A",
                "pixels_per_inch": round(pixels_per_unit / 0.0254, 2) if real_unit == "meters" else "N/A"
            }
        }
        
        return json.dumps(result, indent=2)
        
    except Exception as e:
        return json.dumps({"error": f"Scale calibration failed: {str(e)}"})

@tool
def analyze_object_proportions(image_path: str, objects_json: str, target_object: str) -> str:
    """
    Analyze proportions and relationships between objects for design validation.
    """
    try:
        detected_objects = json.loads(objects_json)
        
        # Find target objects
        target_objects = [obj for obj in detected_objects 
                         if target_object.lower() in obj.get('class_name', '').lower()]
        
        if not target_objects:
            return json.dumps({"error": f"No '{target_object}' objects found"})
        
        proportions_analysis = []
        
        for obj in target_objects:
            bbox = obj['bbox']
            width = abs(bbox[2] - bbox[0])
            height = abs(bbox[3] - bbox[1])
            
            aspect_ratio = width / height if height > 0 else 0
            
            # Analyze proportions
            if 0.9 <= aspect_ratio <= 1.1:
                shape = "Square"
            elif aspect_ratio > 1.1:
                shape = "Horizontal Rectangle"
            else:
                shape = "Vertical Rectangle"
            
            # Golden ratio check
            golden_ratio = 1.618
            golden_deviation = abs(aspect_ratio - golden_ratio) / golden_ratio
            
            proportions_analysis.append({
                "object_id": obj.get('class_name', 'unknown'),
                "aspect_ratio": round(aspect_ratio, 3),
                "shape_classification": shape,
                "golden_ratio_deviation": f"{round(golden_deviation * 100, 1)}%",
                "width_px": round(width, 2),
                "height_px": round(height, 2),
                "design_notes": _get_design_notes(target_object, aspect_ratio)
            })
        
        return json.dumps({
            "success": True,
            "proportions_analysis": proportions_analysis,
            "target_object": target_object
        }, indent=2)
        
    except Exception as e:
        return json.dumps({"error": f"Proportion analysis failed: {str(e)}"})

def _get_design_notes(object_type, aspect_ratio):
    """Provide design guidance based on object type and proportions."""
    object_type_lower = object_type.lower()
    
    if 'steel' in object_type_lower and 'base' in object_type_lower:
        if 0.8 <= aspect_ratio <= 1.2:
            return "Good proportion for base plate - provides balanced load distribution"
        elif aspect_ratio > 1.5:
            return "Consider additional stiffeners for long base plates"
        else:
            return "Verify anchor bolt placement for this aspect ratio"
    
    elif 'door' in object_type_lower:
        if 0.4 <= aspect_ratio <= 0.45:
            return "Standard door proportion (0.9x2.1m)"
        else:
            return "Non-standard door proportion - check accessibility requirements"
    
    elif 'window' in object_type_lower:
        if 1.5 <= aspect_ratio <= 2.5:
            return "Good window proportion for natural light"
        else:
            return "Consider window operation type with this proportion"
    
    return "Proportion appears reasonable for the object type"

@tool
def clean_temp_image(image_path: str):
    """Clean up temporary image files by removing them from the filesystem."""
    if os.path.exists(image_path):
            try:
                os.remove(image_path)
                print(f"DEBUG: Cleaned up temporary image file: {image_path}")
            except Exception as e:
                print(f"DEBUG: Error cleaning up temporary image file {image_path}: {e}")


# List of all available tools
ALL_TOOLS = [
    load_pdf_for_floorplan,
    convert_pdf_page_to_image,
    detect_floor_plan_objects,
    verify_detections,
    internet_search,
    generate_frontend_annotations,
    answer_question_using_rag,
    answer_question_with_suggestions,
    analyze_pdf_page_multimodal,
    measure_objects,
    calibrate_scale,
    analyze_object_proportions,
    clean_temp_image
]